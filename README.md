### The "95% Signal" AI Reading List


This resource is designed to serve as a **single, unified, and carefully curated map of the most important research papers in modern artificial intelligence**, spanning the foundational breakthroughs of the past decade to the cutting-edge advancements of the 2024–2025 frontier. The goal is to bring together, in one place, the essential works that every researcher, engineer, or curious newcomer should understand in order to gain a deep and structured grasp of how today’s most powerful AI systems are built, trained, scaled, aligned, and deployed.

Rather than forcing learners to navigate thousands of scattered papers, blog posts, and technical reports, this list distills the field down to **the core ideas that have shaped contemporary AI**:

* the invention of the Transformer and attention-based architectures;
* the philosophy and empirical evidence behind large-scale training;
* modern optimization and training techniques such as DPO, RL-based reasoning, and GRPO;
* breakthroughs in efficient attention (FlashAttention), state-space models (Mamba), and scalable inference;
* the rise of agentic systems, system-2 reasoning, and long-horizon RL for LLMs;
* the emergence of next-generation generative modeling paradigms such as Flow Matching;
* and the philosophical frameworks—like the Bitter Lesson and Scaling Laws—that explain *why* the field evolves the way it does.

This document aims to be **a high-signal introduction to the field**, giving newcomers the ability to rapidly understand not only *what* the most influential papers are, but also *why they matter*, how they connect to one another, and how they underpin the capabilities of models like GPT-4+, Claude, DeepSeek-R1, Llama 3, Kimi K2, Sora, Stable Diffusion 3, Flux, and the next generation of agentic AI systems.

By reading through the works collected here, a newcomer can trace the complete arc of modern AI—from first principles and foundational architectures to emergent reasoning, agentic behavior, and frontier-scale training techniques—without needing prior expertise or years of background research. The intention is to make the path into AI research **clearer, faster, and intellectually grounded**, and to serve as a living reference that evolves with the state of the art.

If you’re beginning your journey into AI research or seeking to deepen your understanding of how today’s most advanced systems work, this collection provides the **most important conceptual building blocks** in one comprehensive, navigable place.


## Papers


| Name                                                                 | Comprehensive Description                                                                                                                                                                                                                                                                             | Link                                                                                                        |
| -------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------- |
| **Attention Is All You Need (2017)**                                 | The paper that introduced the **Transformer**, replacing recurrence and convolution with self-attention. It became the foundation of all modern LLMs, diffusion transformers, and multimodal models. Practically every architecture today is a scaling, refinement, or reinterpretation of this work. | [Arxiv](https://arxiv.org/abs/1706.03762)                                                                   |
| **Adam Optimizer (2014)**                                            | The default optimizer for deep learning. Introduced adaptive moment estimation, enabling stable training of large neural networks and serving as the base for AdamW and LAMB used in today’s foundation models.                                                                                       | [Arxiv](https://arxiv.org/abs/1412.6980)                                                                    |
| **The Bitter Lesson (2019)**                                         | Sutton's philosophical cornerstone: methods that scale with computation always outperform hand-engineered approaches. Forms the conceptual foundation for modern scaling laws, RL-based reasoning, and agentic training regimes.                                                                      | [Essay](http://www.incompleteideas.net/IncIdeas/BitterLesson.html)                                          |
| **Scaling Laws for Neural Language Models (GPT Scaling Laws, 2020)** | Kaplan et al. empirically demonstrated predictable power-law relationships between compute, data, and performance. Provided the blueprint for all subsequent model scaling efforts (GPT-3, Chinchilla, Llama 3, DeepSeek family).                                                                     | [Arxiv](https://arxiv.org/abs/2001.08361)                                                                   |
| **Welcome to the Era of Experience (2025)**                          | Silver & Sutton’s manifesto declaring a shift from data-driven to **experience-driven** AI. Argues that future systems will rely on agents learning from their own interactions, not human-curated corpora. Influences all work on agentic LLMs and system-2 reasoning.                               | [PDF](https://www.google.com/search?q=http://incompleteideas.net/IncIdeas/SilverSutton2025.pdf)             |
| **Kimi K2: Open Agentic Intelligence (2025)**                        | Technical report for Moonshot’s trillion-parameter MoE model built explicitly for agentic behavior. Introduces architecture and training strategies tailored for **slow thinking**, tool use, and extended reasoning. A major milestone in agent LLM design.                                          | [Arxiv](https://arxiv.org/abs/2507.20534)                                                                   |
| **DeepSeekMath (GRPO Source, 2024)**                                 | Introduced **GRPO (Group Relative Policy Optimization)**—a stable, compute-efficient RL algorithm for training reasoning models without a value function. GRPO became the backbone of DeepSeek R1 and other RL-trained reasoning LLMs.                                                                | [Arxiv](https://arxiv.org/abs/2402.03300)                                                                   |
| **DeepSeek-R1 Technical Report (2025)**                              | Landmark model demonstrating **RL-first reasoning at scale**. R1 showed that long-chain deliberate reasoning can be trained directly via RL without supervised CoT. Sparked the shift toward reinforcement-dominant training pipelines.                                                               | [Arxiv](https://arxiv.org/abs/2501.12579)                                                                   |
| **Tree of Thoughts (ToT, 2023)**                                     | Introduces search-over-thoughts: a structured reasoning approach using branch-and-bound on intermediate thoughts. Served as the conceptual precursor to many modern agentic planners and reasoning frameworks (MCTS-CoT, R1 search loops).                                                            | [Arxiv](https://arxiv.org/abs/2305.10601)                                                                   |
| **FLUX.1 Kontext & Flow Matching (2022–2024)**                       | “Flow Matching for Generative Modeling” established a new generative modeling paradigm that supersedes classic diffusion. Kontext extends this to **in-context image generation**, enabling editing, slot-filling, and consistent style/structure transformations. Forms the basis of FLUX.1 and SD3. | FM: [Arxiv](https://arxiv.org/abs/2210.02747), Kontext: [GitHub](https://github.com/black-forest-labs/flux) |
| **Scalable Diffusion Models with Transformers (DiT, 2022)**          | Replaces U-Nets with pure Transformers for diffusion, enabling massive scaling in image/video generation (Sora, SD3, Flux, Lumina). Established the **Diffusion Transformer** as the dominant image backbone.                                                                                         | [Arxiv](https://arxiv.org/abs/2212.09748)                                                                   |
| **Direct Preference Optimization (DPO, 2023)**                       | Made RLHF dramatically simpler and more stable by removing the need for a reward model. Became the standard for preference training and alignment across open-source and commercial models.                                                                                                           | [Arxiv](https://arxiv.org/abs/2305.18290)                                                                   |
| **Mamba 1 (2023): Selective State Spaces**                           | A state-space model with linear-time sequence processing that rivals transformer performance. Enables extremely long context windows with lower compute. Sparked renewed interest in RNN-like architectures.                                                                                          | [Arxiv](https://arxiv.org/abs/2312.00752)                                                                   |
| **Mamba-2 / Transformers are SSMs (2024)**                           | Unified the Transformer and SSM views through Structured State Space Duality. Introduced GPU-efficient kernels and improvements that make Mamba architectures more scalable and competitive with LLM-scale training.                                                                                  | [Arxiv](https://arxiv.org/abs/2405.21060)                                                                   |
| **GQA: Generalized Multi-Query Attention (2023)**                    | Efficient attention design used in Llama 2/3, DeepSeek, and many optimized inference stacks. Balances the speed of MQA with the quality of MHA by grouping query heads.                                                                                                                               | [Arxiv](https://arxiv.org/abs/2305.13245)                                                                   |
| **FlashAttention 1 & 2 (2022–2023)**                                 | A high-performance attention kernel that minimizes memory reads/writes. FlashAttention enabled training very large models efficiently, making long-context attention feasible and becoming the standard kernel for LLMs.                                                                              | FA1: [Arxiv](https://arxiv.org/abs/2205.14135), FA2: [Arxiv](https://arxiv.org/abs/2307.08691)              |
